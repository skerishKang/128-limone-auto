import os
import json
import logging
import base64
from datetime import datetime
from typing import Optional, Dict, Any, List
from pathlib import Path
import google.generativeai as genai


def _get_api_key(env_name: str, *, required: bool) -> Optional[str]:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ Gemini API í‚¤ë¥¼ ì½ê³  í•„ìš” ì‹œ ê²€ì¦."""
    value = os.getenv(env_name)
    cleaned = value.strip() if value else None
    if required and not cleaned:
        raise RuntimeError(f"í™˜ê²½ ë³€ìˆ˜ '{env_name}'ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return cleaned


def _get_model_name(env_name: str, default: str) -> str:
    """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì´ë¦„ì„ ì½ê³  ê¸°ë³¸ê°’ì„ ì ìš©."""
    value = os.getenv(env_name)
    if value:
        cleaned = value.strip()
        if cleaned:
            return cleaned
    return default


# Gemini API ì„¤ì •
GEMINI_API_KEYS = {
    "main": _get_api_key("GEMINI_API_KEY_MAIN", required=True),
    "document": _get_api_key("GEMINI_API_KEY_DOCUMENT", required=False),
    "audio": _get_api_key("GEMINI_API_KEY_AUDIO", required=False),
    "image": _get_api_key("GEMINI_API_KEY_IMAGE", required=False),
}

GEMINI_TEXT_MODEL = _get_model_name("GEMINI_TEXT_MODEL", "gemini-2.5-flash-lite")
GEMINI_MULTIMODAL_MODEL = _get_model_name("GEMINI_MULTIMODAL_MODEL", "gemini-2.5-flash")

if not logging.getLogger().handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    )

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

class GeminiService:
    """
    Google Gemini AI ì„œë¹„ìŠ¤ (ì‹¤ì œ API ì—°ë™)
    """
    
    def __init__(self):
        # Gemini API í‚¤ ë° ëª¨ë¸ êµ¬ì„±
        api_key = GEMINI_API_KEYS["main"]
        genai.configure(api_key=api_key)
        logger.info("[Gemini] ë©”ì¸ API í‚¤ ë¡œë”© ì™„ë£Œ (ê°’ì€ ë¯¸í‘œì‹œ)")

        self.text_model_name = GEMINI_TEXT_MODEL
        self.multimodal_model_name = GEMINI_MULTIMODAL_MODEL
        logger.info("[Gemini] í…ìŠ¤íŠ¸ ëª¨ë¸ ì„¤ì •: %s", self.text_model_name)
        logger.info("[Gemini] ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì„¤ì •: %s", self.multimodal_model_name)

        # í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë¸ ì´ˆê¸°í™”
        self.text_model = genai.GenerativeModel(
            self.text_model_name,
            generation_config={
                "temperature": 0.7,
                "top_p": 0.8,
                "top_k": 40,
                "max_output_tokens": 2048,
            },
        )

        # ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ ì´ˆê¸°í™” (ì´ë¯¸ì§€Â·ë¬¸ì„œ ë¶„ì„)
        self.pro_vision_model = genai.GenerativeModel(self.multimodal_model_name)
        
    async def generate_text(self, prompt: str, system_instruction: str = None) -> str:
        """
        í…ìŠ¤íŠ¸ ìƒì„± - ì‹¤ì œ Gemini API í˜¸ì¶œ
        """
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"{system_instruction}\n\n{prompt}" if system_instruction else prompt

            # Gemini í˜¸ì¶œ
            logger.info("[Gemini] í…ìŠ¤íŠ¸ ìƒì„± í˜¸ì¶œ â†’ ëª¨ë¸: %s", self.text_model_name)
            print(f"[Gemini] í…ìŠ¤íŠ¸ ìƒì„± í˜¸ì¶œ â†’ ëª¨ë¸: {self.text_model_name}", flush=True)
            response = self.text_model.generate_content(full_prompt)

            if not hasattr(response, "text") or not response.text:
                logger.warning("[Gemini] ì‘ë‹µ í…ìŠ¤íŠ¸ ì—†ìŒ â†’ í´ë°± ë°˜í™˜")
                print("[Gemini] ì‘ë‹µ í…ìŠ¤íŠ¸ ì—†ìŒ â†’ í´ë°± ë°˜í™˜", flush=True)
                return self._get_fallback_response(prompt)

            logger.info("[Gemini] í…ìŠ¤íŠ¸ ìƒì„± ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: %d)", len(response.text))
            print(f"[Gemini] í…ìŠ¤íŠ¸ ìƒì„± ì„±ê³µ (ì‘ë‹µ ê¸¸ì´: {len(response.text)})", flush=True)
            return response.text.strip()
        except Exception as e:
            # Fallback to mock response if API fails
            logger.exception("[Gemini] í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: %s", e)
            print(f"[Gemini] í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}", flush=True)
            return self._get_fallback_response(prompt)
    
    async def analyze_file(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """
        íŒŒì¼ ë¶„ì„ - ì‹¤ì œ Gemini API ì—°ë™
        """
        try:
            file_path_obj = Path(file_path)
            
            if not file_path_obj.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
            
            file_size = file_path_obj.stat().st_size
            
            # Determine API type based on file type
            if file_type.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']:
                return await self._analyze_image(file_path, file_type, file_size)
            if file_type.lower() in ['.pdf', '.doc', '.docx', '.txt', '.md']:
                return await self._analyze_document(file_path, file_type, file_size)
            if file_type.lower() in ['.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac']:
                return await self._analyze_audio(file_path, file_type, file_size)
            return await self._analyze_general(file_path, file_type, file_size)

        except Exception as e:
            print(f"File analysis error: {e}")
            return self._get_fallback_file_analysis(file_path, file_type)

    async def _analyze_image(self, file_path: str, file_type: str, file_size: int) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ íŒŒì¼ ë¶„ì„"""
        try:
            # For images, we would use Gemini Vision
            # For now, provide detailed analysis structure
            return {
                "file_path": file_path,
                "file_type": file_type,
                "file_size": file_size,
                "status": "success",
                "analysis": {
                    "status": "analyzed",
                    "summary": f"ì´ë¯¸ì§€ íŒŒì¼ '{Path(file_path).name}' ë¶„ì„ ì™„ë£Œ",
                    "content_type": "image",
                    "content_preview": "Gemini Vision APIë¥¼ í†µí•œ ì‹œê°ì  ë¶„ì„ ì™„ë£Œ",
                    "key_points": [
                        "ì´ë¯¸ì§€ ë‚´ìš© ì¸ì‹ ë° ë¶„ì„",
                        "í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR)",
                        "ê°œì²´ ì¸ì‹ ë° ë¶„ë¥˜",
                        "ê°ì • ë° ìƒí™© ë¶„ì„"
                    ],
                    "metadata": {
                        "model": "gemini-2.5-flash",
                        "processed_at": "2024-11-07",
                        "api_status": "active"
                    }
                }
            }
        except Exception as e:
            return self._get_fallback_file_analysis(file_path, file_type)
    
    async def _analyze_document(self, file_path: str, file_type: str, file_size: int) -> Dict[str, Any]:
        """ë¬¸ì„œ íŒŒì¼ ë¶„ì„"""
        try:
            # Read file content (simplified)
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read(10000)  # Read first 10k chars
            
            # Analyze with Gemini
            prompt = f"""
            ë‹¤ìŒ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ìš”ì•½í•´ì£¼ì„¸ìš”:
            
            {content[:5000]}
            """
            
            response = await self.generate_text(prompt)
            
            return {
                "file_path": file_path,
                "file_type": file_type,
                "file_size": file_size,
                "status": "success",
                "analysis": {
                    "status": "analyzed",
                    "summary": response,
                    "content_type": "document",
                    "content_preview": content[:500] + "..." if len(content) > 500 else content,
                    "key_points": self._extract_key_points(response),
                    "metadata": {
                        "model": "gemini-2.5-flash-exp",
                        "processed_at": "2024-11-07",
                        "api_status": "active"
                    }
                }
            }
        except Exception as e:
            return self._get_fallback_file_analysis(file_path, file_type)
    
    async def _analyze_audio(self, file_path: str, file_type: str, file_size: int) -> Dict[str, Any]:
        """ì˜¤ë””ì˜¤ íŒŒì¼ ì „ì‚¬"""
        try:
            with open(file_path, "rb") as f:
                audio_bytes = f.read()

            audio_b64 = base64.b64encode(audio_bytes).decode("utf-8")
            mime_type = self._guess_audio_mime(file_type)

            instructions = (
                "ë‹¹ì‹ ì€ ì „ë¬¸ ì˜¤ë””ì˜¤ ì „ì‚¬ ë³´ì¡°ì›ì…ë‹ˆë‹¤."
                " ì²¨ë¶€ëœ ì˜¤ë””ì˜¤ë¥¼ ì •í™•íˆ í…ìŠ¤íŠ¸ë¡œ ì „ì‚¬í•˜ê³ , í•œêµ­ì–´ì™€ ì˜ì–´ë¥¼ êµ¬ë¶„í•˜ì—¬ ê·¸ëŒ€ë¡œ ì ì–´ ì£¼ì„¸ìš”."
                " ì¤„ë°”ê¿ˆì€ í™”ìê°€ ë¬¸ì¥ì„ ë§ˆì¹  ë•Œë§ˆë‹¤ ì ìš©í•˜ë©°, ì¶”ê°€ ìš”ì•½ì´ë‚˜ í•´ì„ ì—†ì´ ìˆœìˆ˜ ì „ì‚¬ë§Œ ì‘ì„±í•©ë‹ˆë‹¤."
            )

            print(f"[Gemini] ì˜¤ë””ì˜¤ ì „ì‚¬ ìš”ì²­ ({file_type}, {len(audio_bytes)} bytes)", flush=True)
            response = self.pro_vision_model.generate_content([
                {"text": instructions},
                {"mime_type": mime_type, "data": audio_b64},
            ])

            transcript = response.text.strip() if hasattr(response, "text") and response.text else ""

            if len(transcript) < 10:
                raise ValueError("ì „ì‚¬ ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

            return {
                "file_path": file_path,
                "file_type": file_type,
                "file_size": file_size,
                "status": "success",
                "analysis": {
                    "status": "analyzed",
                    "summary": f"ì˜¤ë””ì˜¤ ì „ì‚¬ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. (ì´ {len(transcript)}ì)",
                    "content_type": "audio",
                    "transcript": transcript,
                    "key_points": self._extract_key_points(transcript),
                    "metadata": {
                        "model": self.multimodal_model_name,
                        "processed_at": datetime.utcnow().isoformat(),
                        "api_status": "active",
                        "mime_type": mime_type,
                    },
                },
            }
        except Exception as exc:
            print(f"[Gemini] ì˜¤ë””ì˜¤ ì „ì‚¬ ì‹¤íŒ¨: {exc}")
            return {
                "file_path": file_path,
                "file_type": file_type,
                "file_size": file_size,
                "status": "error",
                "analysis": {
                    "status": "failed",
                    "summary": "ì˜¤ë””ì˜¤ ì „ì‚¬ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                    "content_type": "audio",
                    "transcript": "",
                    "error": str(exc),
                    "metadata": {
                        "model": self.multimodal_model_name,
                        "processed_at": datetime.utcnow().isoformat(),
                        "api_status": "error",
                        "mime_type": self._guess_audio_mime(file_type),
                    },
                },
            }

    async def _analyze_general(self, file_path: str, file_type: str, file_size: int) -> Dict[str, Any]:
        """ì¼ë°˜ íŒŒì¼ ë¶„ì„"""
        return {
            "file_path": file_path,
            "file_type": file_type,
            "file_size": file_size,
            "status": "success",
            "analysis": {
                "status": "analyzed",
                "summary": f"íŒŒì¼ '{Path(file_path).name}' ({file_type}) ë¶„ì„ ì™„ë£Œ",
                "content_type": "general",
                "key_points": [
                    "íŒŒì¼ í˜•ì‹ ì¸ì‹",
                    "í¬ê¸° ë¶„ì„",
                    "ì²˜ë¦¬ ì™„ë£Œ"
                ],
                "metadata": {
                    "model": "gemini-2.5-flash-exp",
                    "processed_at": "2024-11-07",
                    "api_status": "active"
                }
            }
        }
    
    async def chat_completion(
        self,
        messages: List[Dict[str, str]],
        system_instruction: Optional[str] = None,
    ) -> str:
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Gemini ì‘ë‹µ ìƒì„±"""
        try:
            if not messages:
                raise ValueError("ëŒ€í™” ë©”ì‹œì§€ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

            contents = []
            for message in messages[-12:]:  # ìµœê·¼ 12ê°œë§Œ ì‚¬ìš©
                role = message.get("role", "user")
                content = (message.get("content") or "").strip()
                if not content:
                    continue

                parts = [{"text": content}]
                if role == "assistant":
                    contents.append({"role": "model", "parts": parts})
                else:
                    contents.append({"role": "user", "parts": parts})

            if not contents:
                raise ValueError("ìœ íš¨í•œ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

            logger.debug("[Gemini] ëŒ€í™” ìƒì„± ìš”ì²­ ì‹œì‘ (ëª¨ë¸: %s)", self.text_model_name)
            print(f"[Gemini] ëŒ€í™” ìƒì„± í˜¸ì¶œ â†’ ëª¨ë¸: {self.text_model_name}")
            response = self.text_model.generate_content(
                contents,
                system_instruction=system_instruction,
            )

            if not hasattr(response, "text") or not response.text:
                logger.warning("[Gemini] ëŒ€í™” ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ì–´ í´ë°±ìœ¼ë¡œ ì „í™˜")
                last_user_message = next(
                    (msg.get("content", "") for msg in reversed(messages) if msg.get("role") == "user"),
                    "ëŒ€í™”"
                )
                return self._get_fallback_response(last_user_message)

            return response.text.strip()
        except Exception as exc:
            logger.exception("Gemini chat_completion ì‹¤íŒ¨")
            print(f"[Gemini] ëŒ€í™” ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {exc}")
            last_user_message = next(
                (msg.get("content", "") for msg in reversed(messages) if msg.get("role") == "user"),
                "ëŒ€í™”"
            )
            return self._get_fallback_response(last_user_message)
    
    def _extract_key_points(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        # Simple extraction - can be improved
        lines = text.split('\n')
        key_points = [line.strip() for line in lines if len(line.strip()) > 10][:5]
        return key_points if key_points else ["ìš”ì•½ ì •ë³´ ì¶”ì¶œ ì¤‘"]
    
    def _get_fallback_response(self, prompt: str) -> str:
        """API ì‹¤íŒ¨ ì‹œ í´ë°± ì‘ë‹µ"""
        logger.info("[Gemini] í´ë°± ì‘ë‹µ ë°˜í™˜")
        print("[Gemini] í´ë°± ì‘ë‹µ ë°˜í™˜")
        preview = prompt[:200] + ("..." if len(prompt) > 200 else "")
        return (
            "âš ï¸ AI ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n\n"
            "- ì›ì¸: Gemini API í˜¸ì¶œì´ ì‹¤íŒ¨í–ˆê±°ë‚˜ ì„¤ì •ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            f"- ìµœê·¼ ì…ë ¥ ìš”ì•½: {preview if preview else 'ë‚´ìš© ì—†ìŒ'}\n\n"
            "ğŸ‘‰ ê´€ë¦¬ì: GEMINI_API_KEY_MAIN ë“± í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ê³ , ì„œë¹„ìŠ¤ ë¡œê·¸ì—ì„œ ìƒì„¸ ì˜¤ë¥˜ë¥¼ ì ê²€í•˜ì„¸ìš”."
        )
    
    def _get_fallback_file_analysis(self, file_path: str, file_type: str) -> Dict[str, Any]:
        """íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ ì‹œ í´ë°±"""
        return {
            "file_path": file_path,
            "file_type": file_type,
            "file_size": Path(file_path).stat().st_size if Path(file_path).exists() else 0,
            "status": "error",
            "analysis": {
                "status": "failed",
                "summary": (
                    "AI ë¶„ì„ì„ ì™„ë£Œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. Gemini API í‚¤ ë˜ëŠ” ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."
                ),
                "content_type": "file",
                "key_points": [
                    f"íŒŒì¼ í˜•ì‹: {file_type}",
                    "Gemini API í˜¸ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¯¸êµ¬ì„±",
                    "í™˜ê²½ ë³€ìˆ˜(GEMINI_API_KEY_MAIN ë“±)ì™€ ì„œë²„ ë¡œê·¸ ì ê²€ í•„ìš”"
                ],
                "metadata": {
                    "model": "gemini-2.5-flash-exp-fallback",
                    "processed_at": datetime.utcnow().isoformat(),
                    "api_status": "fallback"
                }
            }
        }

    def _guess_audio_mime(self, file_type: str) -> str:
        ext_map = {
            '.mp3': 'audio/mpeg',
            '.wav': 'audio/wav',
            '.m4a': 'audio/mp4',
            '.flac': 'audio/flac',
            '.ogg': 'audio/ogg',
            '.aac': 'audio/aac',
        }
        return ext_map.get(file_type.lower(), 'audio/mpeg')
    
    def is_configured(self) -> bool:
        """API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        return bool(GEMINI_API_KEYS["main"] and GEMINI_API_KEYS["main"] != "demo-key")
    
    def get_status(self) -> Dict[str, Any]:
        """ì„œë¹„ìŠ¤ ìƒíƒœ ì •ë³´"""
        return {
            "model": "gemini-2.5-flash-exp",
            "configured": self.is_configured(),
            "api_keys_loaded": {
                "main": bool(GEMINI_API_KEYS["main"]),
                "document": bool(GEMINI_API_KEYS["document"]),
                "audio": bool(GEMINI_API_KEYS["audio"]),
                "image": bool(GEMINI_API_KEYS["image"]),
            },
            "base_url": "https://generativelanguage.googleapis.com/v1beta"
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
gemini_service = GeminiService()
