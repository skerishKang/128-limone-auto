import asyncio
from uuid import uuid4
from fastapi import APIRouter, HTTPException, UploadFile, File
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any, Literal, Union
from pathlib import Path
import io
from datetime import datetime

from services.file_processor import file_processor
from services.drive_service import (
    drive_service,
    DriveAuthorizationError,
    DriveAPIError,
)

router = APIRouter()

UPLOAD_DIR = file_processor.upload_dir
SUMMARIES_DIR = Path(__file__).resolve().parents[2] / "summaries"
SUMMARIES_DIR.mkdir(parents=True, exist_ok=True)

PROCESSING_TASKS: Dict[str, Dict[str, Any]] = {}


class FileMetadata(BaseModel):
    stored_name: str
    original_name: str
    mime_type: Optional[str]
    size: int
    category: str
    path: str


class AnalysisPayload(BaseModel):
    summary: Optional[str] = None
    content_type: Optional[str] = None
    key_points: List[str] = Field(default_factory=list)
    metadata: Optional[Dict[str, Any]] = None
    raw: Dict[str, Any] = Field(default_factory=dict)


class FileAnalysisResult(BaseModel):
    success: bool
    message: str
    file: FileMetadata
    analysis: AnalysisPayload
    summary_path: Optional[str] = None
    drive_upload: Optional[Dict[str, Any]] = None


class FileProcessingStatusResponse(BaseModel):
    task_id: str
    status: Literal['processing', 'completed', 'failed']
    result: Optional[FileAnalysisResult] = None
    error: Optional[str] = None
    filename: Optional[str] = None


class FileUploadProcessingResponse(BaseModel):
    task_id: str
    status: Literal['processing']
    message: str
    check_endpoint: str
    filename: str
    estimated_time: Optional[str] = "30-60ì´ˆ"


class FileUploadCompletedResponse(BaseModel):
    status: Literal['completed']
    message: str
    filename: str
    result: Optional[FileAnalysisResult] = None


FileUploadResponse = Union[FileUploadProcessingResponse, FileUploadCompletedResponse]


def categorize_file(file_ext: Optional[str], mime_type: Optional[str]) -> str:
    ext = (file_ext or "").lower()
    mime = (mime_type or "").lower()

    image_types = {".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"}
    document_types = {".pdf", ".doc", ".docx", ".txt", ".md", ".csv", ".json", ".rtf"}
    audio_types = {".mp3", ".wav", ".m4a", ".flac", ".ogg", ".aac"}

    if ext in image_types or mime.startswith("image/"):
        return "image"
    if ext in document_types or mime.startswith("application/") or mime.startswith("text/"):
        return "document"
    if ext in audio_types or mime.startswith("audio/"):
        return "audio"
    return "other"


def save_summary_to_markdown(filename: str, category: str, analysis_result: str) -> Path:
    """ë¶„ì„ ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥"""
    summary_filename = f"{Path(filename).stem}_{category}_ai_summary.md"
    summary_path = SUMMARIES_DIR / summary_filename

    markdown_content = f"""# {filename} - AI ë¶„ì„ ê²°ê³¼

## ğŸ“ íŒŒì¼ ì •ë³´
- **íŒŒì¼ëª…**: {filename}
- **ì¹´í…Œê³ ë¦¬**: {category.upper()}
- **ë¶„ì„ì¼**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ¤– AI ë¶„ì„ ê²°ê³¼

{analysis_result}

---
*ì´ ë¶„ì„ì€ Google Gemini AIë¥¼ ì‚¬ìš©í•˜ì—¬ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""

    summary_path.write_text(markdown_content, encoding="utf-8")
    return summary_path


async def _run_file_analysis(saved: Dict[str, Any]) -> FileAnalysisResult:
    analysis_result = await file_processor.process_file(saved["file_path"])

    analysis_payload = analysis_result.get("analysis", {})
    raw_payload = analysis_result.get("raw_result", {})

    category = analysis_payload.get("content_type") or categorize_file(
        saved.get("file_type"),
        saved.get("mime_type"),
    )

    summary_text = (analysis_payload or {}).get("summary")
    summary_path = None
    if summary_text:
        summary_path = save_summary_to_markdown(
            saved["original_name"],
            category,
            summary_text,
        )

    drive_upload_info: Optional[Dict[str, Any]] = None
    if category == "other":
        try:
            with open(saved["file_path"], "rb") as f:
                buffer = io.BytesIO(f.read())
                buffer.seek(0)
            uploaded = await drive_service.upload_file(
                stream=buffer,
                filename=saved["original_name"],
                mime_type=saved.get("mime_type"),
            )
            drive_upload_info = {
                "success": True,
                "file_id": uploaded.get("id"),
                "name": uploaded.get("name"),
                "webViewLink": uploaded.get("webViewLink"),
                "webContentLink": uploaded.get("webContentLink"),
            }
        except DriveAuthorizationError as exc:
            drive_upload_info = {
                "success": False,
                "error": str(exc),
                "requires_auth": True,
            }
        except DriveAPIError as exc:
            drive_upload_info = {
                "success": False,
                "error": str(exc),
            }
        except Exception as exc:  # pragma: no cover - ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë¡œê¹…ìš©
            drive_upload_info = {
                "success": False,
                "error": f"Drive ì—…ë¡œë“œ ì‹¤íŒ¨: {exc}",
            }

    return FileAnalysisResult(
        success=analysis_result.get("success", True),
        message=analysis_result.get("message", "íŒŒì¼ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."),
        file=FileMetadata(
            stored_name=saved["filename"],
            original_name=saved["original_name"],
            mime_type=saved.get("mime_type"),
            size=saved["file_size"],
            category=category,
            path=saved["file_path"],
        ),
        analysis=AnalysisPayload(
            summary=summary_text,
            content_type=analysis_payload.get("content_type"),
            key_points=analysis_payload.get("key_points", []),
            metadata=analysis_payload.get("metadata"),
            raw=raw_payload,
        ),
        summary_path=str(summary_path) if summary_path else None,
        drive_upload=drive_upload_info,
    )


async def _process_file_task(task_id: str, saved: Dict[str, Any]) -> None:
    try:
        result = await _run_file_analysis(saved)
        PROCESSING_TASKS[task_id]["status"] = "completed"
        result_payload = result.model_dump()
        result_payload.setdefault("status", "success" if result.success else "failed")
        PROCESSING_TASKS[task_id]["result"] = result_payload
        PROCESSING_TASKS[task_id]["completed_at"] = datetime.utcnow().isoformat()
    except Exception as exc:  # pragma: no cover - ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë¡œê¹…ìš©
        PROCESSING_TASKS[task_id]["status"] = "failed"
        PROCESSING_TASKS[task_id]["error"] = str(exc)
        PROCESSING_TASKS[task_id]["completed_at"] = datetime.utcnow().isoformat()


@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(file: UploadFile = File(...)):
    """ë©€í‹°ëª¨ë‹¬ AI íŒŒì¼ ì—…ë¡œë“œ (ë¹„ë™ê¸° ë¶„ì„)"""
    try:
        saved = await file_processor.save_upload(file)
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {exc}") from exc

    category = categorize_file(saved.get("file_type"), saved.get("mime_type"))

    if category == "image":
        immediate_result = FileAnalysisResult(
            success=True,
            message="ì´ë¯¸ì§€ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            file=FileMetadata(
                stored_name=saved["filename"],
                original_name=saved.get("original_name", saved["filename"]),
                mime_type=saved.get("mime_type"),
                size=saved.get("file_size", 0),
                category=category,
                path=saved["file_path"],
            ),
            analysis=AnalysisPayload(
                summary=None,
                content_type="image",
                key_points=[],
                metadata=None,
                raw={
                    "note": "ì´ë¯¸ì§€ íŒŒì¼ì€ ì—…ë¡œë“œë§Œ ì§€ì›ë˜ë©° AI ë¶„ì„ì€ ì¶”í›„ ì œê³µë©ë‹ˆë‹¤.",
                },
            ),
            summary_path=None,
            drive_upload=None,
        )

        return FileUploadCompletedResponse(
            status="completed",
            message="ì´ë¯¸ì§€ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            filename=saved.get("original_name", saved.get("filename")),
            result=immediate_result,
        )

    task_id = uuid4().hex
    PROCESSING_TASKS[task_id] = {
        "status": "processing",
        "saved": saved,
        "filename": saved.get("original_name"),
        "started_at": datetime.utcnow().isoformat(),
    }

    asyncio.create_task(_process_file_task(task_id, saved))

    return FileUploadProcessingResponse(
        task_id=task_id,
        status="processing",
        message="íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ. AI ë¶„ì„ì´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.",
        check_endpoint=f"/api/files/status/{task_id}",
        filename=saved.get("original_name", saved.get("filename")),
    )


@router.get("/status/{task_id}", response_model=FileProcessingStatusResponse)
async def get_file_status(task_id: str):
    task = PROCESSING_TASKS.get(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    response: Dict[str, Any] = {
        "task_id": task_id,
        "status": task.get("status", "processing"),
        "filename": task.get("filename"),
    }

    if task["status"] == "completed" and task.get("result"):
        response["result"] = task["result"]
    if task["status"] == "failed":
        response["error"] = task.get("error", "íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

    return response

@router.get("/list")
async def list_files():
    """ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡"""
    try:
        return file_processor.list_files()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list files: {str(e)}")

@router.get("/summaries")
async def list_summaries():
    """AI ë¶„ì„ ê²°ê³¼ ëª©ë¡"""
    try:
        summaries = []
        for summary_path in SUMMARIES_DIR.glob("*.md"):
            summaries.append({
                "filename": summary_path.name,
                "path": str(summary_path),
                "size": summary_path.stat().st_size,
                "created_at": summary_path.stat().st_ctime
            })
        return summaries
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list summaries: {str(e)}")

@router.get("/summary/{filename}")
async def get_summary(filename: str):
    """íŠ¹ì • íŒŒì¼ì˜ ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
    try:
        # íŒŒì¼ëª…ì—ì„œ summary íŒŒì¼ ì°¾ê¸°
        stem = Path(filename).stem
        summary_files = list(SUMMARIES_DIR.glob(f"{stem}_*_ai_summary.md"))

        if not summary_files:
            raise HTTPException(status_code=404, detail="Summary not found")

        summary_path = summary_files[0]
        with open(summary_path, 'r', encoding='utf-8') as f:
            content = f.read()

        return {
            "filename": filename,
            "summary_path": str(summary_path),
            "content": content
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get summary: {str(e)}")

@router.delete("/delete/{filename}")
async def delete_file(filename: str):
    """íŒŒì¼ ë° ê´€ë ¨ ìš”ì•½ ì‚­ì œ"""
    try:
        if file_processor.delete_file(filename):
            file_path = UPLOAD_DIR / filename
            if file_path.exists():
                file_path.unlink()

        # ê´€ë ¨ ìš”ì•½ íŒŒì¼ë„ ì‚­ì œ
        stem = Path(filename).stem
        for summary_path in SUMMARIES_DIR.glob(f"{stem}_*_ai_summary.md"):
            summary_path.unlink()

        return {"message": f"File {filename} and its summary deleted"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete file: {str(e)}")
